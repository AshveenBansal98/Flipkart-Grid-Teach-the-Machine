{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Flipkart.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MnaUvOMJGSKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This notebook was made on google colab.\n",
        "# Make sure to change runtime to GPU. \n",
        "# As the memory limit is 12GB, running all cells together on colab will crash runtime. \n",
        "# So the notebook should be partially run and the session needs to be reset\n",
        "# The cells corresponding to taking the input need to be run everytime the session is started\n",
        "\n",
        "# test and training images were split into two zips\n",
        "# You need to make a copy of the files in your drive and then replace the link with corresponding links of your id\n",
        "\n",
        "# The images were scaled to 48*64 size\n",
        "# The model was tested with 1, 2 and 3 CNN layers and 2 layers were choosen\n",
        "# Then different kernel size were tested and 72 for first and 88 for second layer was choosen\n",
        "# Then different dense layer size were tested and 2048 was choosen\n",
        "# Dropout was tested but it didn't improve result\n",
        "# Converting second CNN layer (88C5) to two CNN layers (88C3-88C3) didn't improved result\n",
        "# 2048 dense layer was split into two, 1024 and 128 gave best result \n",
        "\n",
        "#The initial cells corrsespond to these tests\n",
        "\n",
        "#The model was run for 2000 epochs in two phases.\n",
        "#After training was completed, model with lowest val_loss was selected for prediction\n",
        "#The final prediction values were rounded off to nearest integer\n",
        "#As weights are randomly assigned, it should be run for 2-3 time for best results\n",
        "#TO TEST ONLY FINAL MODEL, SKIP THE CELLS MARKED \"#MODEL TESTING\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQFCmX1C3q1y",
        "colab_type": "code",
        "outputId": "2c9f3467-e8e6-4c84-ae6d-de07a874501d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Link :https://drive.google.com/open?id=10yfJC1Ilkyj0GiOvHDCw9c-HMteOcvlJ\n",
        "file_id = '10yfJC1Ilkyj0GiOvHDCw9c-HMteOcvlJ' #link to train images zip\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('train_images.zip')\n",
        "\n",
        "#Link :https://drive.google.com/open?id=\n",
        "train_id = '14auPVokQCaJ5VbcA3tPap2mfKaGFlnLd' #link to train.csv\n",
        "myfile = drive.CreateFile({'id': train_id})\n",
        "myfile.GetContentFile('file.csv')\n",
        "train = pd.read_csv('file.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 4.7MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 6.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 5.3MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 6.3MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 7.2MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 8.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 8.9MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 7.1MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 7.2MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.8MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 9.7MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 17.1MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 17.2MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 17.2MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 16.8MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 17.0MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 17.0MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 46.0MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 21.2MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 20.9MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 21.7MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 21.7MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 21.5MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 20.6MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 21.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 21.6MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 21.4MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 22.0MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 53.1MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 55.0MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 55.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 49.5MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 49.5MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 55.4MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 53.7MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 53.3MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 29.9MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 27.8MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 27.5MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 27.4MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 27.4MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 28.3MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 28.5MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 28.8MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 29.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 28.6MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 49.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 53.3MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 54.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 54.9MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 54.6MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 58.2MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 58.3MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 57.1MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 58.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 59.9MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 60.4MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 65.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 61.5MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 62.9MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 62.6MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 60.5MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 43.2MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 40.7MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 40.5MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 40.7MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 40.2MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 40.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 41.3MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 40.8MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 41.2MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 41.8MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 58.4MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 64.4MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 63.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 63.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 64.5MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 61.5MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 63.2MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 64.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 64.2MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 56.4MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 55.4MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 55.9MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 57.1MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 57.3MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 56.3MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 58.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 59.0MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 58.9MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 58.4MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 68.2MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 69.0MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 67.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.3MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qAWYmtaBpl95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "train = pd.read_csv('file.csv')\n",
        "zip_ref = zipfile.ZipFile('train_images.zip', 'r')\n",
        "zip_ref.extractall('MODEL/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOQC4xknnxGF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train.iloc[:, 1:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqb0a3Lfn5Sk",
        "colab_type": "code",
        "outputId": "812c89f5-5469-40f9-d1f0-70608832bd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as _Imgdis\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from time import time\n",
        "from time import sleep\n",
        "folder = \"/content/MODEL/train_images/\"\n",
        "train_files = train.iloc[:, 0:1]\n",
        "\n",
        "print(\"Working with {0} images\".format(len(train_files)))\n",
        "train_files.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working with 14000 images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JPEG_20160706_121146_1000145715002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JPEG_20161119_174038_1000690577600.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>147444927651111470309333776-Roadster-Men-Casua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>147772332675720161028_161611.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1473315333651DeeplearnS11638.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image_name\n",
              "0             JPEG_20160706_121146_1000145715002.png\n",
              "1             JPEG_20161119_174038_1000690577600.png\n",
              "2  147444927651111470309333776-Roadster-Men-Casua...\n",
              "3                   147772332675720161028_161611.png\n",
              "4                   1473315333651DeeplearnS11638.png"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "7B2MCgjYrdCz",
        "colab_type": "code",
        "outputId": "269bde65-55c8-4877-e15c-fab32202897d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "image_width = 64\n",
        "image_height = 48\n",
        "channels = 3\n",
        "dataset = np.ndarray(shape=(len(train_files), image_height, image_width, channels),\n",
        "                     dtype=np.float32)\n",
        "i = 0\n",
        "for file in train_files.loc[:, \"image_name\"]:\n",
        "    img = load_img(folder + \"/\" + file)  # this is a PIL image\n",
        "    # Convert to Numpy Array\n",
        "    img.thumbnail((image_width, image_height))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((48, 64, 3))\n",
        "    x = (x - 128.0) / 128.0\n",
        "    dataset[i] = x;\n",
        "    i+=1\n",
        "\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yqqCLSPa0ZDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -R /content/MODEL/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ws76ntq80bAL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm /content/train_images.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZH5GRPsozIZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.01, random_state=37) #test_size was 0.1 for testing various models. 0.01 for final model\n",
        "dataset = np.ndarray(shape=(1, image_height, image_width, channels),dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aM86BvHr6eeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7gqEo3GNaKh",
        "colab_type": "code",
        "outputId": "24a908e2-a6d1-4e91-8fea-31d25e5a300e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        }
      },
      "cell_type": "code",
      "source": [
        "#MODEL TESTING\n",
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(3):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(60,kernel_size=5,padding='same',activation='relu',input_shape=(image_height, image_width, channels)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    if j>0:\n",
        "        model[j].add(Conv2D(60,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D())\n",
        "    if j>1:\n",
        "        model[j].add(Conv2D(60,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D(padding='same'))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(4))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "    model[j].fit(X_train, y_train, batch_size=64, epochs=20, verbose = 1, validation_data = (X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 6s 497us/sample - loss: 9290.9828 - acc: 0.9005 - val_loss: 2802.1430 - val_acc: 0.9050\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 2241.1649 - acc: 0.9110 - val_loss: 2041.9059 - val_acc: 0.9150\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 6s 462us/sample - loss: 1762.3325 - acc: 0.9179 - val_loss: 1715.5944 - val_acc: 0.9186\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 1513.8817 - acc: 0.9250 - val_loss: 1616.7276 - val_acc: 0.9271\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 1355.2508 - acc: 0.9301 - val_loss: 1360.0364 - val_acc: 0.9321\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 1276.2565 - acc: 0.9325 - val_loss: 1303.5008 - val_acc: 0.9329\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 1225.7675 - acc: 0.9358 - val_loss: 1337.2159 - val_acc: 0.9336\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 1138.2826 - acc: 0.9384 - val_loss: 1227.7780 - val_acc: 0.9314\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 6s 466us/sample - loss: 1091.8831 - acc: 0.9392 - val_loss: 1191.4734 - val_acc: 0.9343\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 1020.3199 - acc: 0.9435 - val_loss: 1132.9202 - val_acc: 0.9350\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 971.0794 - acc: 0.9451 - val_loss: 1164.1491 - val_acc: 0.9357\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 944.7217 - acc: 0.9462 - val_loss: 1097.6609 - val_acc: 0.9350\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 901.2887 - acc: 0.9453 - val_loss: 1106.6574 - val_acc: 0.9464\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 868.9004 - acc: 0.9473 - val_loss: 1048.8879 - val_acc: 0.9343\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 836.8102 - acc: 0.9483 - val_loss: 1078.5693 - val_acc: 0.9364\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 810.5227 - acc: 0.9490 - val_loss: 1137.5532 - val_acc: 0.9400\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 761.4913 - acc: 0.9497 - val_loss: 1046.3994 - val_acc: 0.9343\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 729.2041 - acc: 0.9505 - val_loss: 1077.7096 - val_acc: 0.9414\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 697.5050 - acc: 0.9508 - val_loss: 1057.2509 - val_acc: 0.9414\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 677.9010 - acc: 0.9525 - val_loss: 979.2805 - val_acc: 0.9421\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 6s 493us/sample - loss: 9270.7473 - acc: 0.8862 - val_loss: 2473.8077 - val_acc: 0.9107\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 6s 456us/sample - loss: 2143.6821 - acc: 0.9090 - val_loss: 1916.2811 - val_acc: 0.9200\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 6s 455us/sample - loss: 1910.6801 - acc: 0.9121 - val_loss: 1835.9491 - val_acc: 0.9157\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 6s 460us/sample - loss: 1709.3960 - acc: 0.9175 - val_loss: 1702.5314 - val_acc: 0.9236\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 6s 454us/sample - loss: 1632.9590 - acc: 0.9232 - val_loss: 1585.3314 - val_acc: 0.9257\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 6s 466us/sample - loss: 1512.8193 - acc: 0.9260 - val_loss: 1457.4364 - val_acc: 0.9250\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 6s 455us/sample - loss: 1392.9845 - acc: 0.9290 - val_loss: 1481.8967 - val_acc: 0.9329\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 6s 455us/sample - loss: 1299.4325 - acc: 0.9364 - val_loss: 1579.1734 - val_acc: 0.9314\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 6s 467us/sample - loss: 1204.7803 - acc: 0.9385 - val_loss: 1221.2175 - val_acc: 0.9379\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 1120.9774 - acc: 0.9422 - val_loss: 1150.2321 - val_acc: 0.9364\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 1126.7565 - acc: 0.9440 - val_loss: 1586.5343 - val_acc: 0.9393\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 6s 458us/sample - loss: 1064.2712 - acc: 0.9446 - val_loss: 1088.7984 - val_acc: 0.9393\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 6s 460us/sample - loss: 1025.6473 - acc: 0.9471 - val_loss: 1072.3441 - val_acc: 0.9457\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 969.5065 - acc: 0.9475 - val_loss: 1009.4526 - val_acc: 0.9464\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 944.8766 - acc: 0.9495 - val_loss: 980.3446 - val_acc: 0.9479\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 911.8970 - acc: 0.9494 - val_loss: 1017.2145 - val_acc: 0.9464\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 6s 471us/sample - loss: 903.1620 - acc: 0.9510 - val_loss: 954.0631 - val_acc: 0.9514\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 6s 460us/sample - loss: 869.2631 - acc: 0.9498 - val_loss: 947.8311 - val_acc: 0.9429\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 6s 454us/sample - loss: 834.2416 - acc: 0.9520 - val_loss: 935.7329 - val_acc: 0.9486\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 836.8468 - acc: 0.9519 - val_loss: 908.2065 - val_acc: 0.9393\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 6s 509us/sample - loss: 11470.3185 - acc: 0.8993 - val_loss: 3294.3643 - val_acc: 0.9100\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 6s 482us/sample - loss: 2425.1016 - acc: 0.9080 - val_loss: 2076.6979 - val_acc: 0.9136\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 6s 493us/sample - loss: 1920.9900 - acc: 0.9181 - val_loss: 1934.2036 - val_acc: 0.9257\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 6s 488us/sample - loss: 1577.6296 - acc: 0.9257 - val_loss: 1666.1933 - val_acc: 0.9264\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 6s 487us/sample - loss: 1516.9356 - acc: 0.9298 - val_loss: 1423.1455 - val_acc: 0.9336\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 6s 487us/sample - loss: 1393.8893 - acc: 0.9348 - val_loss: 1557.8397 - val_acc: 0.9371\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 6s 487us/sample - loss: 1386.6748 - acc: 0.9365 - val_loss: 1315.6520 - val_acc: 0.9350\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 6s 499us/sample - loss: 1317.2115 - acc: 0.9390 - val_loss: 1258.0608 - val_acc: 0.9436\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 6s 494us/sample - loss: 1256.6774 - acc: 0.9420 - val_loss: 1684.7813 - val_acc: 0.9314\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 6s 487us/sample - loss: 1185.6340 - acc: 0.9428 - val_loss: 1331.3921 - val_acc: 0.9479\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 6s 486us/sample - loss: 1157.4469 - acc: 0.9434 - val_loss: 1138.4732 - val_acc: 0.9507\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 6s 479us/sample - loss: 1133.4048 - acc: 0.9465 - val_loss: 1162.5868 - val_acc: 0.9500\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 6s 482us/sample - loss: 1072.7612 - acc: 0.9475 - val_loss: 1065.1659 - val_acc: 0.9486\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 6s 487us/sample - loss: 1091.5355 - acc: 0.9490 - val_loss: 1067.6850 - val_acc: 0.9500\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 6s 505us/sample - loss: 991.8177 - acc: 0.9491 - val_loss: 1060.4955 - val_acc: 0.9414\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 6s 500us/sample - loss: 1002.5978 - acc: 0.9499 - val_loss: 1002.8880 - val_acc: 0.9521\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 6s 486us/sample - loss: 979.5581 - acc: 0.9506 - val_loss: 1058.8422 - val_acc: 0.9579\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 6s 492us/sample - loss: 956.2004 - acc: 0.9519 - val_loss: 1013.0997 - val_acc: 0.9400\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 6s 492us/sample - loss: 936.3203 - acc: 0.9511 - val_loss: 1330.6847 - val_acc: 0.9543\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 6s 492us/sample - loss: 951.2424 - acc: 0.9512 - val_loss: 1142.0471 - val_acc: 0.9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hZgi4C5IAChP",
        "colab_type": "code",
        "outputId": "ece64869-595d-432c-925f-9ccc3c576292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4199
        }
      },
      "cell_type": "code",
      "source": [
        "#MODEL TESTING\n",
        "nets = 6\n",
        "model = [0] *nets\n",
        "for j in range(6):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(j*8 + 32,kernel_size=5,padding='same',activation='relu',input_shape=(image_height, image_width, channels)))\n",
        "    model[j].add(Conv2D(j*8 + 48,kernel_size=5,padding='same', activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(4))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "    model[j].fit(X_train, y_train, batch_size=64, epochs=20, verbose = 1, validation_data = (X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 9s 721us/sample - loss: 8266.1040 - acc: 0.9025 - val_loss: 2482.8553 - val_acc: 0.9000\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 8s 660us/sample - loss: 2061.5194 - acc: 0.9129 - val_loss: 1933.0520 - val_acc: 0.9064\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 8s 657us/sample - loss: 1761.6067 - acc: 0.9184 - val_loss: 1679.1044 - val_acc: 0.9221\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 8s 649us/sample - loss: 1588.0954 - acc: 0.9241 - val_loss: 1576.2376 - val_acc: 0.9236\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 8s 664us/sample - loss: 1414.7711 - acc: 0.9310 - val_loss: 1595.0392 - val_acc: 0.9314\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 8s 657us/sample - loss: 1327.7798 - acc: 0.9352 - val_loss: 1240.2152 - val_acc: 0.9371\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 8s 657us/sample - loss: 1173.5277 - acc: 0.9389 - val_loss: 1194.5046 - val_acc: 0.9364\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 8s 662us/sample - loss: 1104.9591 - acc: 0.9429 - val_loss: 1323.2842 - val_acc: 0.9364\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 8s 651us/sample - loss: 1063.6526 - acc: 0.9429 - val_loss: 1180.6348 - val_acc: 0.9429\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 8s 665us/sample - loss: 952.3241 - acc: 0.9467 - val_loss: 1033.5875 - val_acc: 0.9350\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 8s 662us/sample - loss: 927.4330 - acc: 0.9466 - val_loss: 1045.2771 - val_acc: 0.9386\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 8s 651us/sample - loss: 891.5019 - acc: 0.9490 - val_loss: 988.3635 - val_acc: 0.9500\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 8s 658us/sample - loss: 873.2720 - acc: 0.9510 - val_loss: 1142.2324 - val_acc: 0.9443\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 8s 664us/sample - loss: 805.8197 - acc: 0.9504 - val_loss: 1134.8226 - val_acc: 0.9393\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 8s 656us/sample - loss: 763.5739 - acc: 0.9531 - val_loss: 951.2684 - val_acc: 0.9421\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 8s 658us/sample - loss: 739.6817 - acc: 0.9540 - val_loss: 969.6422 - val_acc: 0.9414\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 8s 649us/sample - loss: 722.5616 - acc: 0.9552 - val_loss: 947.7166 - val_acc: 0.9429\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 8s 652us/sample - loss: 657.9469 - acc: 0.9552 - val_loss: 984.0227 - val_acc: 0.9479\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 8s 653us/sample - loss: 635.4520 - acc: 0.9562 - val_loss: 970.6711 - val_acc: 0.9479\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 8s 665us/sample - loss: 620.7482 - acc: 0.9579 - val_loss: 939.6556 - val_acc: 0.9471\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 12s 948us/sample - loss: 7197.1002 - acc: 0.9012 - val_loss: 2275.5650 - val_acc: 0.9136\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 11s 868us/sample - loss: 1971.5090 - acc: 0.9135 - val_loss: 1859.8497 - val_acc: 0.9157\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 11s 871us/sample - loss: 1695.6257 - acc: 0.9189 - val_loss: 1620.2417 - val_acc: 0.9193\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 11s 882us/sample - loss: 1477.3441 - acc: 0.9254 - val_loss: 1468.2873 - val_acc: 0.9129\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 1337.1560 - acc: 0.9328 - val_loss: 1296.3971 - val_acc: 0.9343\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 1254.4477 - acc: 0.9373 - val_loss: 1245.9742 - val_acc: 0.9386\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 11s 871us/sample - loss: 1090.3564 - acc: 0.9428 - val_loss: 1250.5440 - val_acc: 0.9364\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 11s 878us/sample - loss: 1048.0533 - acc: 0.9448 - val_loss: 1117.8185 - val_acc: 0.9371\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 11s 871us/sample - loss: 994.9090 - acc: 0.9468 - val_loss: 1070.3238 - val_acc: 0.9343\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 903.7390 - acc: 0.9462 - val_loss: 1080.5878 - val_acc: 0.9393\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 11s 875us/sample - loss: 886.9885 - acc: 0.9475 - val_loss: 1066.6218 - val_acc: 0.9393\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 11s 870us/sample - loss: 800.0958 - acc: 0.9510 - val_loss: 960.7944 - val_acc: 0.9414\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 11s 873us/sample - loss: 736.8667 - acc: 0.9547 - val_loss: 1109.5333 - val_acc: 0.9457\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 11s 864us/sample - loss: 727.8328 - acc: 0.9524 - val_loss: 980.5052 - val_acc: 0.9443\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 11s 880us/sample - loss: 656.2501 - acc: 0.9547 - val_loss: 981.8880 - val_acc: 0.9507\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 11s 887us/sample - loss: 647.2188 - acc: 0.9572 - val_loss: 948.5343 - val_acc: 0.9500\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 11s 884us/sample - loss: 584.5311 - acc: 0.9577 - val_loss: 1082.8756 - val_acc: 0.9521\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 542.7410 - acc: 0.9594 - val_loss: 935.1852 - val_acc: 0.9443\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 512.7330 - acc: 0.9592 - val_loss: 1063.7138 - val_acc: 0.9471\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 11s 883us/sample - loss: 475.8251 - acc: 0.9639 - val_loss: 1004.8237 - val_acc: 0.9400\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 7793.9885 - acc: 0.9027 - val_loss: 2306.1584 - val_acc: 0.9121\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 12s 924us/sample - loss: 2051.3115 - acc: 0.9132 - val_loss: 1889.6982 - val_acc: 0.9157\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 12s 940us/sample - loss: 1779.2459 - acc: 0.9197 - val_loss: 1761.8619 - val_acc: 0.9236\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 12s 934us/sample - loss: 1509.1419 - acc: 0.9303 - val_loss: 1518.6013 - val_acc: 0.9300\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 12s 930us/sample - loss: 1358.4871 - acc: 0.9367 - val_loss: 1365.1489 - val_acc: 0.9407\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 12s 932us/sample - loss: 1235.0733 - acc: 0.9380 - val_loss: 1200.1026 - val_acc: 0.9443\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 12s 924us/sample - loss: 1128.6786 - acc: 0.9429 - val_loss: 1335.9820 - val_acc: 0.9421\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 12s 931us/sample - loss: 1084.1369 - acc: 0.9422 - val_loss: 1213.0958 - val_acc: 0.9443\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 12s 938us/sample - loss: 992.9402 - acc: 0.9470 - val_loss: 1084.1082 - val_acc: 0.9421\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 12s 929us/sample - loss: 942.9151 - acc: 0.9476 - val_loss: 1010.8330 - val_acc: 0.9421\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 12s 931us/sample - loss: 884.7370 - acc: 0.9485 - val_loss: 1021.2568 - val_acc: 0.9400\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 12s 928us/sample - loss: 848.3821 - acc: 0.9517 - val_loss: 1135.6668 - val_acc: 0.9436\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 12s 928us/sample - loss: 814.5627 - acc: 0.9516 - val_loss: 974.8669 - val_acc: 0.9514\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 12s 929us/sample - loss: 745.4954 - acc: 0.9528 - val_loss: 1003.9753 - val_acc: 0.9514\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 12s 932us/sample - loss: 734.7744 - acc: 0.9525 - val_loss: 932.9688 - val_acc: 0.9471\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 12s 928us/sample - loss: 656.4843 - acc: 0.9540 - val_loss: 1059.2237 - val_acc: 0.9450\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 12s 936us/sample - loss: 639.7509 - acc: 0.9561 - val_loss: 982.3331 - val_acc: 0.9500\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 12s 928us/sample - loss: 607.6160 - acc: 0.9581 - val_loss: 1008.1545 - val_acc: 0.9521\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 12s 954us/sample - loss: 552.4217 - acc: 0.9581 - val_loss: 971.1422 - val_acc: 0.9457\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 12s 929us/sample - loss: 512.5958 - acc: 0.9619 - val_loss: 1023.9218 - val_acc: 0.9450\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 7465.7200 - acc: 0.9010 - val_loss: 2204.1539 - val_acc: 0.9164\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1947.6796 - acc: 0.9147 - val_loss: 1777.5432 - val_acc: 0.9186\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1629.8360 - acc: 0.9212 - val_loss: 1474.7533 - val_acc: 0.9250\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1409.8286 - acc: 0.9294 - val_loss: 1419.9483 - val_acc: 0.9307\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1245.4673 - acc: 0.9342 - val_loss: 1297.2180 - val_acc: 0.9307\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1173.1197 - acc: 0.9393 - val_loss: 1219.5156 - val_acc: 0.9350\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 1074.4188 - acc: 0.9419 - val_loss: 1088.6067 - val_acc: 0.9364\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 983.6422 - acc: 0.9449 - val_loss: 1044.5716 - val_acc: 0.9314\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 955.4707 - acc: 0.9464 - val_loss: 1067.8070 - val_acc: 0.9400\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 859.2735 - acc: 0.9466 - val_loss: 1236.5055 - val_acc: 0.9436\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 838.0138 - acc: 0.9521 - val_loss: 948.5979 - val_acc: 0.9400\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 765.6209 - acc: 0.9540 - val_loss: 1022.8011 - val_acc: 0.9436\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 746.4639 - acc: 0.9537 - val_loss: 942.0734 - val_acc: 0.9379\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 711.4099 - acc: 0.9565 - val_loss: 977.1379 - val_acc: 0.9443\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 656.1435 - acc: 0.9577 - val_loss: 1013.0098 - val_acc: 0.9457\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 614.2931 - acc: 0.9565 - val_loss: 998.6893 - val_acc: 0.9479\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 556.6897 - acc: 0.9585 - val_loss: 971.4463 - val_acc: 0.9464\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 520.5834 - acc: 0.9604 - val_loss: 999.6694 - val_acc: 0.9464\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 13s 1ms/sample - loss: 499.6254 - acc: 0.9632 - val_loss: 1110.0497 - val_acc: 0.9471\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 14s 1ms/sample - loss: 464.9690 - acc: 0.9659 - val_loss: 929.5139 - val_acc: 0.9450\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 16s 1ms/sample - loss: 7094.3581 - acc: 0.9049 - val_loss: 2187.7240 - val_acc: 0.9157\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 2054.7471 - acc: 0.9124 - val_loss: 1908.8106 - val_acc: 0.9229\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1737.7771 - acc: 0.9225 - val_loss: 1711.0041 - val_acc: 0.9243\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1497.7091 - acc: 0.9359 - val_loss: 1434.9457 - val_acc: 0.9386\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1283.8682 - acc: 0.9388 - val_loss: 1236.8880 - val_acc: 0.9407\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1165.2235 - acc: 0.9418 - val_loss: 1253.8484 - val_acc: 0.9393\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1117.4482 - acc: 0.9444 - val_loss: 1135.1662 - val_acc: 0.9429\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 1024.4958 - acc: 0.9454 - val_loss: 1070.8281 - val_acc: 0.9393\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 933.3340 - acc: 0.9468 - val_loss: 1118.1540 - val_acc: 0.9457\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 921.4969 - acc: 0.9491 - val_loss: 1036.0752 - val_acc: 0.9464\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 877.8791 - acc: 0.9519 - val_loss: 993.4154 - val_acc: 0.9407\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 845.7949 - acc: 0.9523 - val_loss: 1312.6339 - val_acc: 0.9471\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 750.2932 - acc: 0.9542 - val_loss: 944.1240 - val_acc: 0.9443\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 736.8988 - acc: 0.9545 - val_loss: 1032.6398 - val_acc: 0.9464\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 654.8226 - acc: 0.9563 - val_loss: 914.6529 - val_acc: 0.9507\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 615.9744 - acc: 0.9590 - val_loss: 977.8300 - val_acc: 0.9486\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 595.3414 - acc: 0.9592 - val_loss: 940.4517 - val_acc: 0.9393\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 532.7780 - acc: 0.9625 - val_loss: 954.2765 - val_acc: 0.9536\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 493.2137 - acc: 0.9617 - val_loss: 909.8002 - val_acc: 0.9521\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 15s 1ms/sample - loss: 459.4766 - acc: 0.9637 - val_loss: 1013.2457 - val_acc: 0.9436\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/20\n",
            "12600/12600 [==============================] - 21s 2ms/sample - loss: 7000.7662 - acc: 0.8994 - val_loss: 2481.8914 - val_acc: 0.8957\n",
            "Epoch 2/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1910.5479 - acc: 0.9168 - val_loss: 1768.1581 - val_acc: 0.9193\n",
            "Epoch 3/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1563.3448 - acc: 0.9276 - val_loss: 1590.5505 - val_acc: 0.9379\n",
            "Epoch 4/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1355.2050 - acc: 0.9347 - val_loss: 1347.6690 - val_acc: 0.9414\n",
            "Epoch 5/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1204.1532 - acc: 0.9421 - val_loss: 1172.9808 - val_acc: 0.9421\n",
            "Epoch 6/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1136.9468 - acc: 0.9448 - val_loss: 1125.2077 - val_acc: 0.9386\n",
            "Epoch 7/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 1021.5512 - acc: 0.9455 - val_loss: 1018.0291 - val_acc: 0.9429\n",
            "Epoch 8/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 915.6938 - acc: 0.9479 - val_loss: 1377.9001 - val_acc: 0.9414\n",
            "Epoch 9/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 864.1686 - acc: 0.9509 - val_loss: 985.5133 - val_acc: 0.9393\n",
            "Epoch 10/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 804.1915 - acc: 0.9502 - val_loss: 969.3800 - val_acc: 0.9450\n",
            "Epoch 11/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 758.4759 - acc: 0.9533 - val_loss: 1000.5239 - val_acc: 0.9521\n",
            "Epoch 12/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 678.3593 - acc: 0.9549 - val_loss: 1097.0194 - val_acc: 0.9529\n",
            "Epoch 13/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 644.5091 - acc: 0.9575 - val_loss: 938.2922 - val_acc: 0.9450\n",
            "Epoch 14/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 631.0931 - acc: 0.9600 - val_loss: 908.6789 - val_acc: 0.9500\n",
            "Epoch 15/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 556.0254 - acc: 0.9617 - val_loss: 922.0898 - val_acc: 0.9536\n",
            "Epoch 16/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 507.8345 - acc: 0.9624 - val_loss: 881.5337 - val_acc: 0.9514\n",
            "Epoch 17/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 472.9950 - acc: 0.9644 - val_loss: 896.1448 - val_acc: 0.9557\n",
            "Epoch 18/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 418.4466 - acc: 0.9656 - val_loss: 881.1127 - val_acc: 0.9521\n",
            "Epoch 19/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 391.2146 - acc: 0.9654 - val_loss: 1026.2043 - val_acc: 0.9521\n",
            "Epoch 20/20\n",
            "12600/12600 [==============================] - 18s 1ms/sample - loss: 384.7600 - acc: 0.9691 - val_loss: 1005.0802 - val_acc: 0.9536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k-0VcIkyADEx",
        "colab_type": "code",
        "outputId": "1f329e89-09b9-4d07-d775-d5ee5cd02b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3485
        }
      },
      "cell_type": "code",
      "source": [
        "#MODEL TESTING\n",
        "nets = 4\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(0, 4):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(72,kernel_size=5,activation='relu',input_shape=(image_height, image_width, channels)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(88,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(2**(j+4), activation='relu'))\n",
        "    model[j].add(Dense(4, activation='relu'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "    model[j].fit(X_train, y_train, batch_size=64, epochs=25, verbose = 1, validation_data = (X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 6s 505us/sample - loss: 49910.3618 - acc: 0.8948 - val_loss: 43592.1460 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 6s 437us/sample - loss: 43301.5581 - acc: 0.9034 - val_loss: 42914.7401 - val_acc: 0.9029\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 6s 437us/sample - loss: 42956.0911 - acc: 0.9034 - val_loss: 42741.1792 - val_acc: 0.9029\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 6s 438us/sample - loss: 42820.2681 - acc: 0.9034 - val_loss: 42633.2219 - val_acc: 0.9029\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 6s 438us/sample - loss: 42736.3291 - acc: 0.9034 - val_loss: 42594.5877 - val_acc: 0.9029\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 6s 444us/sample - loss: 42729.0263 - acc: 0.9034 - val_loss: 42626.7588 - val_acc: 0.9029\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 42679.5503 - acc: 0.9034 - val_loss: 42570.1070 - val_acc: 0.9029\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 42645.0144 - acc: 0.9034 - val_loss: 42430.8610 - val_acc: 0.9029\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 6s 443us/sample - loss: 42570.2573 - acc: 0.9034 - val_loss: 42399.5101 - val_acc: 0.9029\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 42518.1756 - acc: 0.9034 - val_loss: 42345.3268 - val_acc: 0.9029\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 6s 446us/sample - loss: 42486.2009 - acc: 0.9034 - val_loss: 42238.5494 - val_acc: 0.9029\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 42430.1307 - acc: 0.9034 - val_loss: 42227.5204 - val_acc: 0.9029\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 6s 456us/sample - loss: 42401.6623 - acc: 0.9034 - val_loss: 42173.4423 - val_acc: 0.9029\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 42355.2856 - acc: 0.9034 - val_loss: 42339.6191 - val_acc: 0.9029\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 6s 450us/sample - loss: 42365.3844 - acc: 0.9034 - val_loss: 42148.9048 - val_acc: 0.9029\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 6s 444us/sample - loss: 42315.7704 - acc: 0.9034 - val_loss: 42147.6581 - val_acc: 0.9029\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 42294.5694 - acc: 0.9034 - val_loss: 42125.3902 - val_acc: 0.9029\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 6s 445us/sample - loss: 42274.5558 - acc: 0.9034 - val_loss: 42096.4057 - val_acc: 0.9029\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 6s 444us/sample - loss: 42258.6985 - acc: 0.9034 - val_loss: 42052.7873 - val_acc: 0.9029\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 6s 439us/sample - loss: 42252.1886 - acc: 0.9034 - val_loss: 42076.0104 - val_acc: 0.9029\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 6s 456us/sample - loss: 42225.5072 - acc: 0.9034 - val_loss: 42044.8184 - val_acc: 0.9029\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 6s 449us/sample - loss: 13941.9111 - acc: 0.8861 - val_loss: 4044.0214 - val_acc: 0.9036\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 6s 443us/sample - loss: 3836.8715 - acc: 0.9101 - val_loss: 3763.2758 - val_acc: 0.9179\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 6s 448us/sample - loss: 3706.5407 - acc: 0.9217 - val_loss: 3696.3497 - val_acc: 0.9243\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 6s 449us/sample - loss: 3693.0809 - acc: 0.9253 - val_loss: 3716.3333 - val_acc: 0.9257\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 6s 486us/sample - loss: 13275.5431 - acc: 0.8924 - val_loss: 5186.7963 - val_acc: 0.9071\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 6s 441us/sample - loss: 4772.2737 - acc: 0.9065 - val_loss: 4645.9559 - val_acc: 0.9079\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 6s 439us/sample - loss: 4445.7122 - acc: 0.9053 - val_loss: 4545.5372 - val_acc: 0.9029\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 4324.9101 - acc: 0.9069 - val_loss: 4363.2225 - val_acc: 0.9107\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 6s 439us/sample - loss: 4209.3506 - acc: 0.9104 - val_loss: 4444.2361 - val_acc: 0.9157\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 6s 442us/sample - loss: 4178.8316 - acc: 0.9132 - val_loss: 4227.9971 - val_acc: 0.9107\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 6s 440us/sample - loss: 4140.9804 - acc: 0.9162 - val_loss: 4137.9798 - val_acc: 0.9171\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 4088.3777 - acc: 0.9180 - val_loss: 4082.5841 - val_acc: 0.9214\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 6s 447us/sample - loss: 4033.4211 - acc: 0.9207 - val_loss: 4047.7711 - val_acc: 0.9257\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 3986.3820 - acc: 0.9233 - val_loss: 3988.8215 - val_acc: 0.9243\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 6s 447us/sample - loss: 3949.0125 - acc: 0.9264 - val_loss: 3933.7450 - val_acc: 0.9286\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 6s 460us/sample - loss: 3907.9035 - acc: 0.9294 - val_loss: 4009.3851 - val_acc: 0.9186\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 3880.2344 - acc: 0.9272 - val_loss: 3843.1147 - val_acc: 0.9300\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 6s 454us/sample - loss: 3836.7000 - acc: 0.9304 - val_loss: 3823.3780 - val_acc: 0.9243\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 6s 440us/sample - loss: 3816.5580 - acc: 0.9314 - val_loss: 3904.5432 - val_acc: 0.9293\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 3773.1723 - acc: 0.9311 - val_loss: 3822.8840 - val_acc: 0.9271\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 6s 441us/sample - loss: 3785.2875 - acc: 0.9294 - val_loss: 3908.6466 - val_acc: 0.9271\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 6s 454us/sample - loss: 3754.3774 - acc: 0.9332 - val_loss: 3785.9515 - val_acc: 0.9250\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 6s 441us/sample - loss: 3726.3452 - acc: 0.9339 - val_loss: 3769.2139 - val_acc: 0.9200\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 6s 442us/sample - loss: 3733.8035 - acc: 0.9316 - val_loss: 3758.7779 - val_acc: 0.9286\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 3707.1889 - acc: 0.9353 - val_loss: 3712.6892 - val_acc: 0.9243\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 3676.3253 - acc: 0.9378 - val_loss: 3839.0522 - val_acc: 0.9307\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 6s 446us/sample - loss: 3644.5059 - acc: 0.9370 - val_loss: 3742.0393 - val_acc: 0.9329\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 6s 447us/sample - loss: 3624.4569 - acc: 0.9394 - val_loss: 3808.8070 - val_acc: 0.9200\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 6s 458us/sample - loss: 3615.4071 - acc: 0.9393 - val_loss: 3607.0485 - val_acc: 0.9329\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 6s 500us/sample - loss: 47232.1930 - acc: 0.8533 - val_loss: 40977.8525 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 6s 438us/sample - loss: 40762.9663 - acc: 0.9034 - val_loss: 40381.3428 - val_acc: 0.9029\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 6s 451us/sample - loss: 40477.8395 - acc: 0.9034 - val_loss: 40293.0666 - val_acc: 0.9029\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 6s 461us/sample - loss: 40359.4503 - acc: 0.9034 - val_loss: 40206.5008 - val_acc: 0.9029\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 6s 451us/sample - loss: 40284.3432 - acc: 0.9034 - val_loss: 40120.7550 - val_acc: 0.9029\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 40262.8599 - acc: 0.9034 - val_loss: 40080.5988 - val_acc: 0.9029\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 6s 443us/sample - loss: 40183.2758 - acc: 0.9034 - val_loss: 40241.2380 - val_acc: 0.9029\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 40104.5293 - acc: 0.9034 - val_loss: 39872.9531 - val_acc: 0.9029\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 6s 438us/sample - loss: 40021.3103 - acc: 0.9034 - val_loss: 39802.8368 - val_acc: 0.9029\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 39996.2799 - acc: 0.9034 - val_loss: 39788.1826 - val_acc: 0.9029\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 6s 444us/sample - loss: 39934.6094 - acc: 0.9034 - val_loss: 39740.0703 - val_acc: 0.9029\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 6s 451us/sample - loss: 39874.3031 - acc: 0.9034 - val_loss: 39658.0689 - val_acc: 0.9029\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 6s 439us/sample - loss: 39851.9134 - acc: 0.9034 - val_loss: 39635.7060 - val_acc: 0.9029\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 6s 456us/sample - loss: 39808.7195 - acc: 0.9034 - val_loss: 39610.3881 - val_acc: 0.9029\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 39790.6348 - acc: 0.9034 - val_loss: 39537.6202 - val_acc: 0.9029\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 6s 445us/sample - loss: 39754.6258 - acc: 0.9034 - val_loss: 39605.8279 - val_acc: 0.9029\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 6s 463us/sample - loss: 39736.4454 - acc: 0.9034 - val_loss: 39591.6809 - val_acc: 0.9029\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 6s 438us/sample - loss: 39693.1307 - acc: 0.9034 - val_loss: 39527.9208 - val_acc: 0.9029\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 39683.2555 - acc: 0.9034 - val_loss: 39448.6945 - val_acc: 0.9029\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 6s 449us/sample - loss: 39676.3838 - acc: 0.9034 - val_loss: 39440.0076 - val_acc: 0.9029\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 6s 452us/sample - loss: 39644.0986 - acc: 0.9034 - val_loss: 39442.8314 - val_acc: 0.9029\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 39619.9631 - acc: 0.9034 - val_loss: 39475.2331 - val_acc: 0.9029\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 6s 455us/sample - loss: 39625.6768 - acc: 0.9034 - val_loss: 39440.0355 - val_acc: 0.9029\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 39584.5401 - acc: 0.9034 - val_loss: 39403.7302 - val_acc: 0.9029\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 6s 457us/sample - loss: 39594.5144 - acc: 0.9034 - val_loss: 39401.1894 - val_acc: 0.9029\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 6s 512us/sample - loss: 10076.8498 - acc: 0.9023 - val_loss: 2731.0585 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 6s 453us/sample - loss: 2262.3658 - acc: 0.9077 - val_loss: 2135.6908 - val_acc: 0.8971\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 6s 466us/sample - loss: 2016.5903 - acc: 0.9085 - val_loss: 2176.8311 - val_acc: 0.9086\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 6s 474us/sample - loss: 1840.8758 - acc: 0.9115 - val_loss: 1875.5469 - val_acc: 0.9079\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 1711.8121 - acc: 0.9171 - val_loss: 1753.0297 - val_acc: 0.9236\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 6s 472us/sample - loss: 1602.9381 - acc: 0.9207 - val_loss: 1692.5947 - val_acc: 0.9329\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 6s 480us/sample - loss: 1499.9400 - acc: 0.9287 - val_loss: 1554.1911 - val_acc: 0.9343\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 6s 470us/sample - loss: 1400.5987 - acc: 0.9347 - val_loss: 1415.0647 - val_acc: 0.9386\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 6s 462us/sample - loss: 1347.9502 - acc: 0.9359 - val_loss: 1298.1524 - val_acc: 0.9407\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 1274.6781 - acc: 0.9409 - val_loss: 1223.1827 - val_acc: 0.9429\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 6s 459us/sample - loss: 1199.5341 - acc: 0.9411 - val_loss: 1301.4002 - val_acc: 0.9436\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 6s 473us/sample - loss: 1132.9446 - acc: 0.9467 - val_loss: 1139.2825 - val_acc: 0.9400\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 6s 482us/sample - loss: 1135.1764 - acc: 0.9463 - val_loss: 1160.5918 - val_acc: 0.9414\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 6s 461us/sample - loss: 1059.4629 - acc: 0.9471 - val_loss: 1081.6594 - val_acc: 0.9514\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 6s 472us/sample - loss: 1028.3997 - acc: 0.9484 - val_loss: 1039.5560 - val_acc: 0.9471\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 6s 474us/sample - loss: 980.7810 - acc: 0.9490 - val_loss: 1005.8499 - val_acc: 0.9507\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 6s 472us/sample - loss: 963.5310 - acc: 0.9505 - val_loss: 1053.6103 - val_acc: 0.9450\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 6s 468us/sample - loss: 918.0150 - acc: 0.9510 - val_loss: 1101.9271 - val_acc: 0.9457\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 6s 462us/sample - loss: 923.5094 - acc: 0.9512 - val_loss: 957.3974 - val_acc: 0.9479\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 6s 461us/sample - loss: 881.5497 - acc: 0.9525 - val_loss: 916.4830 - val_acc: 0.9507\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 6s 471us/sample - loss: 864.3934 - acc: 0.9517 - val_loss: 993.7921 - val_acc: 0.9514\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 6s 464us/sample - loss: 833.7071 - acc: 0.9532 - val_loss: 940.4416 - val_acc: 0.9436\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 806.7145 - acc: 0.9523 - val_loss: 935.7572 - val_acc: 0.9514\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 6s 476us/sample - loss: 785.5095 - acc: 0.9551 - val_loss: 893.9380 - val_acc: 0.9557\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 6s 465us/sample - loss: 763.9541 - acc: 0.9542 - val_loss: 942.3550 - val_acc: 0.9521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y3om-0QbfH4i",
        "colab_type": "code",
        "outputId": "d75ee813-ab5d-421f-f41b-513ea5f40725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3485
        }
      },
      "cell_type": "code",
      "source": [
        "#MODEL TESTING\n",
        "nets = 4\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(0, 4):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(72,kernel_size=5,activation='relu',input_shape=(image_height, image_width, channels)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(88,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(2**(j+8), activation='relu'))\n",
        "    model[j].add(Dense(4, activation='relu'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "    model[j].fit(X_train, y_train, batch_size=64, epochs=25, verbose = 1, validation_data = (X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 7s 543us/sample - loss: 8867.2142 - acc: 0.8787 - val_loss: 2521.0313 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 6s 488us/sample - loss: 2164.8041 - acc: 0.9079 - val_loss: 2056.9725 - val_acc: 0.9150\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 6s 489us/sample - loss: 1880.0093 - acc: 0.9116 - val_loss: 2138.2900 - val_acc: 0.9157\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 6s 489us/sample - loss: 1766.8082 - acc: 0.9164 - val_loss: 1883.7000 - val_acc: 0.9200\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 6s 484us/sample - loss: 1608.5838 - acc: 0.9220 - val_loss: 1648.5730 - val_acc: 0.9229\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 6s 493us/sample - loss: 1498.8953 - acc: 0.9266 - val_loss: 1390.8563 - val_acc: 0.9300\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 6s 476us/sample - loss: 1386.9753 - acc: 0.9308 - val_loss: 1359.3289 - val_acc: 0.9357\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 6s 488us/sample - loss: 1285.8297 - acc: 0.9358 - val_loss: 1393.1901 - val_acc: 0.9321\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 6s 488us/sample - loss: 1245.3778 - acc: 0.9385 - val_loss: 1205.3690 - val_acc: 0.9357\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 6s 491us/sample - loss: 1154.4634 - acc: 0.9406 - val_loss: 1201.6673 - val_acc: 0.9436\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 6s 481us/sample - loss: 1144.2675 - acc: 0.9444 - val_loss: 1175.9508 - val_acc: 0.9379\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 6s 500us/sample - loss: 1123.8501 - acc: 0.9463 - val_loss: 1188.0486 - val_acc: 0.9400\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 6s 481us/sample - loss: 1036.2997 - acc: 0.9464 - val_loss: 1103.3609 - val_acc: 0.9479\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 6s 495us/sample - loss: 1003.5496 - acc: 0.9470 - val_loss: 1069.7837 - val_acc: 0.9471\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 6s 501us/sample - loss: 989.6106 - acc: 0.9483 - val_loss: 1008.1441 - val_acc: 0.9429\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 6s 500us/sample - loss: 952.1429 - acc: 0.9490 - val_loss: 1137.3622 - val_acc: 0.9450\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 6s 500us/sample - loss: 933.6022 - acc: 0.9507 - val_loss: 1193.0021 - val_acc: 0.9443\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 6s 485us/sample - loss: 892.2621 - acc: 0.9511 - val_loss: 944.3290 - val_acc: 0.9514\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 6s 493us/sample - loss: 865.5390 - acc: 0.9519 - val_loss: 1061.5402 - val_acc: 0.9493\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 6s 484us/sample - loss: 847.0797 - acc: 0.9519 - val_loss: 919.5858 - val_acc: 0.9500\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 6s 493us/sample - loss: 860.2953 - acc: 0.9526 - val_loss: 1213.2450 - val_acc: 0.9543\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 6s 505us/sample - loss: 838.9963 - acc: 0.9542 - val_loss: 1143.0797 - val_acc: 0.9500\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 6s 490us/sample - loss: 800.1773 - acc: 0.9549 - val_loss: 958.5082 - val_acc: 0.9550\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 6s 494us/sample - loss: 800.2198 - acc: 0.9551 - val_loss: 894.9567 - val_acc: 0.9557\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 6s 494us/sample - loss: 773.5953 - acc: 0.9562 - val_loss: 1088.8444 - val_acc: 0.9486\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 7s 585us/sample - loss: 9117.8671 - acc: 0.9008 - val_loss: 2739.5659 - val_acc: 0.9107\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 7s 536us/sample - loss: 2198.9757 - acc: 0.9096 - val_loss: 1941.2144 - val_acc: 0.9157\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 7s 538us/sample - loss: 1834.8811 - acc: 0.9150 - val_loss: 1913.8518 - val_acc: 0.9136\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 7s 539us/sample - loss: 1687.3851 - acc: 0.9214 - val_loss: 1698.4289 - val_acc: 0.9207\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 7s 545us/sample - loss: 1572.0021 - acc: 0.9264 - val_loss: 1518.0781 - val_acc: 0.9207\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 7s 539us/sample - loss: 1403.6105 - acc: 0.9346 - val_loss: 1623.5291 - val_acc: 0.9321\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 7s 544us/sample - loss: 1311.6609 - acc: 0.9389 - val_loss: 1263.6881 - val_acc: 0.9371\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 7s 556us/sample - loss: 1263.7149 - acc: 0.9382 - val_loss: 1208.9818 - val_acc: 0.9421\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 7s 541us/sample - loss: 1222.9198 - acc: 0.9429 - val_loss: 1633.6236 - val_acc: 0.9436\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 7s 539us/sample - loss: 1154.3186 - acc: 0.9440 - val_loss: 1078.9875 - val_acc: 0.9450\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 7s 535us/sample - loss: 1116.7725 - acc: 0.9430 - val_loss: 1078.0408 - val_acc: 0.9414\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 7s 557us/sample - loss: 1045.1268 - acc: 0.9475 - val_loss: 1032.2243 - val_acc: 0.9457\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 7s 532us/sample - loss: 1003.5484 - acc: 0.9471 - val_loss: 1026.0403 - val_acc: 0.9364\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 7s 556us/sample - loss: 983.1510 - acc: 0.9494 - val_loss: 1032.3241 - val_acc: 0.9443\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 7s 539us/sample - loss: 932.1567 - acc: 0.9476 - val_loss: 1022.3955 - val_acc: 0.9479\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 7s 528us/sample - loss: 927.7767 - acc: 0.9500 - val_loss: 975.0505 - val_acc: 0.9421\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 7s 527us/sample - loss: 879.7359 - acc: 0.9508 - val_loss: 966.7163 - val_acc: 0.9421\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 7s 546us/sample - loss: 891.6299 - acc: 0.9505 - val_loss: 920.3543 - val_acc: 0.9507\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 7s 550us/sample - loss: 843.8292 - acc: 0.9522 - val_loss: 883.4071 - val_acc: 0.9500\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 7s 537us/sample - loss: 794.7075 - acc: 0.9526 - val_loss: 876.6508 - val_acc: 0.9557\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 7s 535us/sample - loss: 769.3040 - acc: 0.9526 - val_loss: 945.3468 - val_acc: 0.9500\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 7s 550us/sample - loss: 781.0148 - acc: 0.9526 - val_loss: 1058.3761 - val_acc: 0.9471\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 7s 547us/sample - loss: 755.8292 - acc: 0.9536 - val_loss: 857.5473 - val_acc: 0.9486\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 7s 530us/sample - loss: 718.1259 - acc: 0.9557 - val_loss: 915.3118 - val_acc: 0.9464\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 7s 528us/sample - loss: 710.0914 - acc: 0.9533 - val_loss: 837.8943 - val_acc: 0.9550\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 9s 686us/sample - loss: 8038.4997 - acc: 0.9032 - val_loss: 2521.1117 - val_acc: 0.9036\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 8s 608us/sample - loss: 2163.4512 - acc: 0.9106 - val_loss: 1978.1916 - val_acc: 0.9150\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 8s 623us/sample - loss: 1871.7565 - acc: 0.9157 - val_loss: 2041.4162 - val_acc: 0.9150\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 8s 611us/sample - loss: 1668.3737 - acc: 0.9234 - val_loss: 1581.4713 - val_acc: 0.9300\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 8s 635us/sample - loss: 1485.8201 - acc: 0.9279 - val_loss: 1415.7351 - val_acc: 0.9371\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 8s 621us/sample - loss: 1366.3684 - acc: 0.9357 - val_loss: 1299.8345 - val_acc: 0.9386\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 8s 619us/sample - loss: 1270.0544 - acc: 0.9410 - val_loss: 1544.7886 - val_acc: 0.9321\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 8s 614us/sample - loss: 1161.8401 - acc: 0.9418 - val_loss: 1210.6617 - val_acc: 0.9321\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 8s 634us/sample - loss: 1094.6935 - acc: 0.9460 - val_loss: 1064.6694 - val_acc: 0.9429\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 8s 609us/sample - loss: 1051.0994 - acc: 0.9468 - val_loss: 1151.9587 - val_acc: 0.9443\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 8s 621us/sample - loss: 1015.1672 - acc: 0.9482 - val_loss: 997.6756 - val_acc: 0.9436\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 8s 625us/sample - loss: 1018.8573 - acc: 0.9467 - val_loss: 1019.4001 - val_acc: 0.9407\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 8s 627us/sample - loss: 953.0170 - acc: 0.9504 - val_loss: 935.4926 - val_acc: 0.9479\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 8s 625us/sample - loss: 879.3198 - acc: 0.9505 - val_loss: 993.8326 - val_acc: 0.9464\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 8s 634us/sample - loss: 880.6244 - acc: 0.9501 - val_loss: 1012.1817 - val_acc: 0.9464\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 8s 615us/sample - loss: 837.9086 - acc: 0.9522 - val_loss: 986.1444 - val_acc: 0.9464\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 8s 635us/sample - loss: 813.5030 - acc: 0.9542 - val_loss: 838.6318 - val_acc: 0.9493\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 8s 611us/sample - loss: 780.6826 - acc: 0.9536 - val_loss: 870.7674 - val_acc: 0.9486\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 8s 623us/sample - loss: 764.8385 - acc: 0.9552 - val_loss: 869.5961 - val_acc: 0.9543\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 8s 628us/sample - loss: 778.2952 - acc: 0.9560 - val_loss: 1060.9735 - val_acc: 0.9464\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 8s 626us/sample - loss: 709.9043 - acc: 0.9544 - val_loss: 828.8068 - val_acc: 0.9514\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 8s 620us/sample - loss: 668.6213 - acc: 0.9560 - val_loss: 869.9574 - val_acc: 0.9564\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 8s 634us/sample - loss: 631.0165 - acc: 0.9573 - val_loss: 1039.8904 - val_acc: 0.9400\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 8s 606us/sample - loss: 604.9405 - acc: 0.9579 - val_loss: 931.7938 - val_acc: 0.9586\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 8s 616us/sample - loss: 594.0693 - acc: 0.9596 - val_loss: 855.8064 - val_acc: 0.9543\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 11s 851us/sample - loss: 45077.8817 - acc: 0.8988 - val_loss: 41093.2373 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 10s 798us/sample - loss: 40510.3604 - acc: 0.9034 - val_loss: 40468.5146 - val_acc: 0.9029\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 10s 790us/sample - loss: 33580.6210 - acc: 0.8852 - val_loss: 2186.9260 - val_acc: 0.9029\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 10s 805us/sample - loss: 1649.4516 - acc: 0.9167 - val_loss: 1573.8264 - val_acc: 0.9221\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 10s 802us/sample - loss: 1426.5503 - acc: 0.9282 - val_loss: 1795.8978 - val_acc: 0.9229\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 10s 802us/sample - loss: 1246.9782 - acc: 0.9337 - val_loss: 1267.5873 - val_acc: 0.9364\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 10s 810us/sample - loss: 1177.0945 - acc: 0.9386 - val_loss: 1151.4495 - val_acc: 0.9357\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 10s 802us/sample - loss: 1078.5641 - acc: 0.9438 - val_loss: 1059.1049 - val_acc: 0.9429\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 10s 803us/sample - loss: 1022.5513 - acc: 0.9459 - val_loss: 1109.8812 - val_acc: 0.9457\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 10s 799us/sample - loss: 971.4755 - acc: 0.9487 - val_loss: 1148.6272 - val_acc: 0.9386\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 10s 793us/sample - loss: 994.8384 - acc: 0.9478 - val_loss: 1053.5916 - val_acc: 0.9479\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 10s 791us/sample - loss: 892.1307 - acc: 0.9511 - val_loss: 1046.6167 - val_acc: 0.9479\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 10s 797us/sample - loss: 867.8175 - acc: 0.9502 - val_loss: 979.6332 - val_acc: 0.9493\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 10s 795us/sample - loss: 821.3665 - acc: 0.9529 - val_loss: 961.9627 - val_acc: 0.9293\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 10s 795us/sample - loss: 779.7249 - acc: 0.9513 - val_loss: 962.3989 - val_acc: 0.9500\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 10s 807us/sample - loss: 777.7241 - acc: 0.9543 - val_loss: 894.8939 - val_acc: 0.9471\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 10s 798us/sample - loss: 732.1304 - acc: 0.9536 - val_loss: 884.2748 - val_acc: 0.9443\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 10s 799us/sample - loss: 676.2790 - acc: 0.9570 - val_loss: 908.4889 - val_acc: 0.9486\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 10s 799us/sample - loss: 690.4641 - acc: 0.9542 - val_loss: 829.6339 - val_acc: 0.9514\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 10s 796us/sample - loss: 638.4391 - acc: 0.9558 - val_loss: 967.3165 - val_acc: 0.9507\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 10s 794us/sample - loss: 596.8171 - acc: 0.9557 - val_loss: 817.5238 - val_acc: 0.9543\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 10s 804us/sample - loss: 583.9550 - acc: 0.9581 - val_loss: 1096.5776 - val_acc: 0.9507\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 10s 796us/sample - loss: 561.2669 - acc: 0.9577 - val_loss: 834.6150 - val_acc: 0.9536\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 10s 799us/sample - loss: 505.4048 - acc: 0.9610 - val_loss: 898.9554 - val_acc: 0.9507\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 10s 795us/sample - loss: 491.0439 - acc: 0.9603 - val_loss: 799.9582 - val_acc: 0.9557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pmUysVv9PE-A",
        "colab_type": "code",
        "outputId": "c1e28e34-bd66-4e0f-82fd-96ccaa3b24f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2903
        }
      },
      "cell_type": "code",
      "source": [
        "#MODEL TESTING\n",
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(0, 4):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(72,kernel_size=5,activation='relu',input_shape=(image_height, image_width, channels)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(0.1 + j*0.1))\n",
        "    model[j].add(Conv2D(88,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(0.1 + j*0.1))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(2048, activation='relu'))\n",
        "    model[j].add(Dropout(0.1 + j*0.1))\n",
        "    model[j].add(Dense(4, activation='relu'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "    model[j].fit(X_train, y_train, batch_size=64, epochs=25, verbose = 1, validation_data = (X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 12s 921us/sample - loss: 8931.9151 - acc: 0.8696 - val_loss: 2979.5025 - val_acc: 0.9029\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 11s 852us/sample - loss: 2278.7406 - acc: 0.9075 - val_loss: 2344.2341 - val_acc: 0.9050\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 11s 856us/sample - loss: 1972.1963 - acc: 0.9127 - val_loss: 1869.2830 - val_acc: 0.9143\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 11s 863us/sample - loss: 1764.6066 - acc: 0.9156 - val_loss: 1561.7902 - val_acc: 0.9179\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 11s 859us/sample - loss: 1584.0119 - acc: 0.9209 - val_loss: 1563.6164 - val_acc: 0.9193\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 11s 856us/sample - loss: 1446.2138 - acc: 0.9288 - val_loss: 1960.4790 - val_acc: 0.9193\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 11s 856us/sample - loss: 1359.0251 - acc: 0.9351 - val_loss: 1157.7757 - val_acc: 0.9364\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 11s 858us/sample - loss: 1276.8744 - acc: 0.9374 - val_loss: 1284.3671 - val_acc: 0.9371\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 11s 858us/sample - loss: 1176.5908 - acc: 0.9406 - val_loss: 1107.2209 - val_acc: 0.9443\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 1147.4330 - acc: 0.9428 - val_loss: 1124.0672 - val_acc: 0.9407\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 1107.7971 - acc: 0.9462 - val_loss: 1122.5822 - val_acc: 0.9421\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 11s 864us/sample - loss: 1047.3979 - acc: 0.9452 - val_loss: 1117.3925 - val_acc: 0.9457\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 11s 858us/sample - loss: 1023.9929 - acc: 0.9464 - val_loss: 930.9438 - val_acc: 0.9464\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 11s 856us/sample - loss: 967.6835 - acc: 0.9474 - val_loss: 1088.2134 - val_acc: 0.9400\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 11s 871us/sample - loss: 963.6778 - acc: 0.9483 - val_loss: 911.1606 - val_acc: 0.9436\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 947.7925 - acc: 0.9501 - val_loss: 995.4732 - val_acc: 0.9500\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 11s 866us/sample - loss: 891.5209 - acc: 0.9491 - val_loss: 1108.8888 - val_acc: 0.9464\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 11s 852us/sample - loss: 933.8766 - acc: 0.9460 - val_loss: 895.7927 - val_acc: 0.9486\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 11s 855us/sample - loss: 879.6203 - acc: 0.9490 - val_loss: 993.4288 - val_acc: 0.9464\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 11s 864us/sample - loss: 835.9231 - acc: 0.9498 - val_loss: 900.3366 - val_acc: 0.9514\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 11s 859us/sample - loss: 806.9858 - acc: 0.9499 - val_loss: 925.4997 - val_acc: 0.9514\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 11s 860us/sample - loss: 815.4443 - acc: 0.9515 - val_loss: 808.7872 - val_acc: 0.9464\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 11s 857us/sample - loss: 741.5674 - acc: 0.9514 - val_loss: 972.0458 - val_acc: 0.9571\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 724.3091 - acc: 0.9524 - val_loss: 931.3535 - val_acc: 0.9579\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 11s 854us/sample - loss: 710.5773 - acc: 0.9519 - val_loss: 901.6685 - val_acc: 0.9550\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 12s 934us/sample - loss: 7407.2920 - acc: 0.9050 - val_loss: 2434.3361 - val_acc: 0.9164\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 11s 857us/sample - loss: 2128.5525 - acc: 0.9148 - val_loss: 2458.7672 - val_acc: 0.9250\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 11s 859us/sample - loss: 1812.3705 - acc: 0.9230 - val_loss: 1598.0270 - val_acc: 0.9314\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 11s 877us/sample - loss: 1560.2845 - acc: 0.9290 - val_loss: 3149.2701 - val_acc: 0.9407\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 11s 864us/sample - loss: 1400.7930 - acc: 0.9365 - val_loss: 2100.2686 - val_acc: 0.9364\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 11s 858us/sample - loss: 1284.7434 - acc: 0.9413 - val_loss: 1837.3049 - val_acc: 0.9436\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 1192.0262 - acc: 0.9464 - val_loss: 1647.8072 - val_acc: 0.9407\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 11s 867us/sample - loss: 1179.0734 - acc: 0.9462 - val_loss: 1329.9080 - val_acc: 0.9393\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 11s 866us/sample - loss: 1132.2095 - acc: 0.9479 - val_loss: 1726.5141 - val_acc: 0.9443\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 11s 879us/sample - loss: 1068.6604 - acc: 0.9480 - val_loss: 1270.4696 - val_acc: 0.9500\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 1050.2300 - acc: 0.9485 - val_loss: 1923.6644 - val_acc: 0.9479\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 11s 870us/sample - loss: 1045.8128 - acc: 0.9512 - val_loss: 1703.7187 - val_acc: 0.9507\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 11s 859us/sample - loss: 961.0057 - acc: 0.9502 - val_loss: 1352.4739 - val_acc: 0.9543\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 11s 870us/sample - loss: 968.8752 - acc: 0.9510 - val_loss: 1060.1193 - val_acc: 0.9443\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 11s 866us/sample - loss: 944.3724 - acc: 0.9502 - val_loss: 896.9215 - val_acc: 0.9536\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 11s 868us/sample - loss: 979.0933 - acc: 0.9524 - val_loss: 859.8915 - val_acc: 0.9464\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 11s 867us/sample - loss: 882.0439 - acc: 0.9536 - val_loss: 1067.3000 - val_acc: 0.9514\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 11s 856us/sample - loss: 832.4822 - acc: 0.9546 - val_loss: 1181.9609 - val_acc: 0.9564\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 859.1734 - acc: 0.9557 - val_loss: 1450.4422 - val_acc: 0.9550\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 810.8642 - acc: 0.9514 - val_loss: 964.9726 - val_acc: 0.9514\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 11s 850us/sample - loss: 773.5655 - acc: 0.9551 - val_loss: 1231.6010 - val_acc: 0.9457\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 775.5214 - acc: 0.9543 - val_loss: 996.1918 - val_acc: 0.9536\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 11s 855us/sample - loss: 775.4072 - acc: 0.9567 - val_loss: 1124.5548 - val_acc: 0.9536\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 11s 855us/sample - loss: 694.6224 - acc: 0.9558 - val_loss: 889.0287 - val_acc: 0.9600\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 681.3056 - acc: 0.9611 - val_loss: 1093.9022 - val_acc: 0.9571\n",
            "Train on 12600 samples, validate on 1400 samples\n",
            "Epoch 1/25\n",
            "12600/12600 [==============================] - 12s 918us/sample - loss: 8007.7688 - acc: 0.9023 - val_loss: 3532.7687 - val_acc: 0.9107\n",
            "Epoch 2/25\n",
            "12600/12600 [==============================] - 11s 858us/sample - loss: 2244.3541 - acc: 0.9107 - val_loss: 2275.3466 - val_acc: 0.9150\n",
            "Epoch 3/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 1948.1790 - acc: 0.9163 - val_loss: 2059.5587 - val_acc: 0.9229\n",
            "Epoch 4/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 1735.2554 - acc: 0.9219 - val_loss: 1980.2121 - val_acc: 0.9314\n",
            "Epoch 5/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 1589.2048 - acc: 0.9314 - val_loss: 2470.3346 - val_acc: 0.9329\n",
            "Epoch 6/25\n",
            "12600/12600 [==============================] - 11s 855us/sample - loss: 1480.9280 - acc: 0.9358 - val_loss: 2494.5397 - val_acc: 0.9421\n",
            "Epoch 7/25\n",
            "12600/12600 [==============================] - 11s 853us/sample - loss: 1388.3366 - acc: 0.9400 - val_loss: 1662.1848 - val_acc: 0.9421\n",
            "Epoch 8/25\n",
            "12600/12600 [==============================] - 11s 874us/sample - loss: 1432.3904 - acc: 0.9406 - val_loss: 2382.7774 - val_acc: 0.9443\n",
            "Epoch 9/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 1258.3151 - acc: 0.9421 - val_loss: 1318.9029 - val_acc: 0.9436\n",
            "Epoch 10/25\n",
            "12600/12600 [==============================] - 11s 861us/sample - loss: 1249.0814 - acc: 0.9444 - val_loss: 1118.1103 - val_acc: 0.9493\n",
            "Epoch 11/25\n",
            "12600/12600 [==============================] - 11s 851us/sample - loss: 1184.8734 - acc: 0.9463 - val_loss: 1782.5323 - val_acc: 0.9500\n",
            "Epoch 12/25\n",
            "12600/12600 [==============================] - 11s 850us/sample - loss: 1193.4632 - acc: 0.9462 - val_loss: 1443.7810 - val_acc: 0.9486\n",
            "Epoch 13/25\n",
            "12600/12600 [==============================] - 11s 864us/sample - loss: 1195.7558 - acc: 0.9459 - val_loss: 1188.2008 - val_acc: 0.9493\n",
            "Epoch 14/25\n",
            "12600/12600 [==============================] - 11s 862us/sample - loss: 1116.8472 - acc: 0.9467 - val_loss: 1937.9941 - val_acc: 0.9507\n",
            "Epoch 15/25\n",
            "12600/12600 [==============================] - 11s 860us/sample - loss: 1133.2915 - acc: 0.9489 - val_loss: 1829.3578 - val_acc: 0.9457\n",
            "Epoch 16/25\n",
            "12600/12600 [==============================] - 11s 846us/sample - loss: 1059.3879 - acc: 0.9498 - val_loss: 1518.8719 - val_acc: 0.9529\n",
            "Epoch 17/25\n",
            "12600/12600 [==============================] - 11s 857us/sample - loss: 1037.9197 - acc: 0.9504 - val_loss: 2433.0807 - val_acc: 0.9507\n",
            "Epoch 18/25\n",
            "12600/12600 [==============================] - 11s 869us/sample - loss: 1055.4785 - acc: 0.9494 - val_loss: 1632.8926 - val_acc: 0.9550\n",
            "Epoch 19/25\n",
            "12600/12600 [==============================] - 11s 857us/sample - loss: 997.0403 - acc: 0.9513 - val_loss: 1161.6419 - val_acc: 0.9586\n",
            "Epoch 20/25\n",
            "12600/12600 [==============================] - 11s 851us/sample - loss: 983.9806 - acc: 0.9520 - val_loss: 1624.6664 - val_acc: 0.9543\n",
            "Epoch 21/25\n",
            "12600/12600 [==============================] - 11s 860us/sample - loss: 972.5063 - acc: 0.9495 - val_loss: 1368.9538 - val_acc: 0.9414\n",
            "Epoch 22/25\n",
            "12600/12600 [==============================] - 11s 849us/sample - loss: 990.9078 - acc: 0.9546 - val_loss: 1268.5676 - val_acc: 0.9364\n",
            "Epoch 23/25\n",
            "12600/12600 [==============================] - 11s 865us/sample - loss: 941.3547 - acc: 0.9518 - val_loss: 1084.0208 - val_acc: 0.9593\n",
            "Epoch 24/25\n",
            "12600/12600 [==============================] - 11s 868us/sample - loss: 951.6685 - acc: 0.9510 - val_loss: 1422.9121 - val_acc: 0.9636\n",
            "Epoch 25/25\n",
            "12600/12600 [==============================] - 11s 851us/sample - loss: 938.8693 - acc: 0.9540 - val_loss: 1387.0577 - val_acc: 0.9564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-126124db64f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "V2TciQNOkkwC",
        "colab_type": "code",
        "outputId": "7e8d09fa-fadc-4085-afd4-0b0507cf268e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34207
        }
      },
      "cell_type": "code",
      "source": [
        "#Phase 1 of training\n",
        "import tensorflow;\n",
        "checkpoint = tensorflow.keras.callbacks.ModelCheckpoint('model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "model = Sequential()\n",
        "model.add(Conv2D(72,kernel_size=5,padding = 'same', activation='relu', input_shape=(image_height, image_width, channels)))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Conv2D(88,kernel_size=5,padding = 'same',activation='relu'))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "model.fit(X_train, y_train, batch_size=100, epochs=1000, verbose = 0, callbacks=[checkpoint], validation_data = (X_test, y_test))\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# model.save_weights('model_weights.h5')\n",
        "weights_file = drive.CreateFile({'title' : 'model_weights1024128.h5'})\n",
        "weights_file.SetContentFile(\"model.h5\")\n",
        "weights_file.Upload()\n",
        "drive.CreateFile({'id': weights_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2386.04851, saving model to model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 2386.04851 to 1532.45633, saving model to model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 1532.45633 to 1436.27522, saving model to model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 1436.27522 to 1191.97450, saving model to model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 1191.97450 to 1085.75746, saving model to model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1085.75746\n",
            "\n",
            "Epoch 00007: val_loss improved from 1085.75746 to 1079.64899, saving model to model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 1079.64899 to 967.61871, saving model to model.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 967.61871 to 883.68935, saving model to model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 883.68935\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 883.68935\n",
            "\n",
            "Epoch 00012: val_loss improved from 883.68935 to 797.59113, saving model to model.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 797.59113 to 749.17879, saving model to model.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 749.17879 to 705.49792, saving model to model.h5\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 705.49792\n",
            "\n",
            "Epoch 00016: val_loss improved from 705.49792 to 674.55449, saving model to model.h5\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 674.55449\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 674.55449\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 674.55449\n",
            "\n",
            "Epoch 00020: val_loss improved from 674.55449 to 607.78783, saving model to model.h5\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 607.78783\n",
            "\n",
            "Epoch 00028: val_loss improved from 607.78783 to 583.29452, saving model to model.h5\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 583.29452\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 583.29452\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 583.29452\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 583.29452\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 583.29452\n",
            "\n",
            "Epoch 00034: val_loss improved from 583.29452 to 576.62585, saving model to model.h5\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 576.62585\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 576.62585\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 576.62585\n",
            "\n",
            "Epoch 00038: val_loss improved from 576.62585 to 549.90860, saving model to model.h5\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 549.90860\n",
            "\n",
            "Epoch 00094: val_loss improved from 549.90860 to 548.85858, saving model to model.h5\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 548.85858\n",
            "\n",
            "Epoch 00104: val_loss improved from 548.85858 to 541.11000, saving model to model.h5\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 541.11000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 541.11000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 541.11000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 541.11000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 541.11000\n",
            "\n",
            "Epoch 00110: val_loss improved from 541.11000 to 528.18486, saving model to model.h5\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 528.18486\n",
            "\n",
            "Epoch 00117: val_loss improved from 528.18486 to 515.79153, saving model to model.h5\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 515.79153\n",
            "\n",
            "Epoch 00138: val_loss improved from 515.79153 to 513.24021, saving model to model.h5\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 513.24021\n",
            "\n",
            "Epoch 00140: val_loss improved from 513.24021 to 511.56923, saving model to model.h5\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 511.56923\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 511.56923\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 511.56923\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 511.56923\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 511.56923\n",
            "\n",
            "Epoch 00146: val_loss improved from 511.56923 to 507.31942, saving model to model.h5\n",
            "\n",
            "Epoch 00147: val_loss improved from 507.31942 to 506.55595, saving model to model.h5\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 506.55595\n",
            "\n",
            "Epoch 00154: val_loss improved from 506.55595 to 499.43066, saving model to model.h5\n",
            "\n",
            "Epoch 00155: val_loss improved from 499.43066 to 498.13932, saving model to model.h5\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 498.13932\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 498.13932\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 498.13932\n",
            "\n",
            "Epoch 00159: val_loss improved from 498.13932 to 489.94080, saving model to model.h5\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 489.94080\n",
            "\n",
            "Epoch 00178: val_loss improved from 489.94080 to 486.93537, saving model to model.h5\n",
            "\n",
            "Epoch 00179: val_loss improved from 486.93537 to 481.69003, saving model to model.h5\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 481.69003\n",
            "\n",
            "Epoch 00201: val_loss improved from 481.69003 to 478.90853, saving model to model.h5\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 478.90853\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 478.90853\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 478.90853\n",
            "\n",
            "Epoch 00205: val_loss improved from 478.90853 to 475.39789, saving model to model.h5\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 475.39789\n",
            "\n",
            "Epoch 00215: val_loss improved from 475.39789 to 468.89987, saving model to model.h5\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 468.89987\n",
            "\n",
            "Epoch 00232: val_loss improved from 468.89987 to 464.06912, saving model to model.h5\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 464.06912\n",
            "\n",
            "Epoch 00234: val_loss improved from 464.06912 to 464.04575, saving model to model.h5\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 464.04575\n",
            "\n",
            "Epoch 00246: val_loss improved from 464.04575 to 460.71783, saving model to model.h5\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 460.71783\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 460.71783\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 460.71783\n",
            "\n",
            "Epoch 00250: val_loss improved from 460.71783 to 456.97820, saving model to model.h5\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 456.97820\n",
            "\n",
            "Epoch 00277: val_loss improved from 456.97820 to 454.29680, saving model to model.h5\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 454.29680\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 454.29680\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 454.29680\n",
            "\n",
            "Epoch 00281: val_loss improved from 454.29680 to 453.31694, saving model to model.h5\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 453.31694\n",
            "\n",
            "Epoch 00289: val_loss improved from 453.31694 to 450.22187, saving model to model.h5\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 450.22187\n",
            "\n",
            "Epoch 00318: val_loss improved from 450.22187 to 438.67681, saving model to model.h5\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 438.67681\n",
            "\n",
            "Epoch 00382: val_loss improved from 438.67681 to 432.10404, saving model to model.h5\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 432.10404\n",
            "\n",
            "Epoch 00480: val_loss improved from 432.10404 to 431.96906, saving model to model.h5\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 431.96906\n",
            "\n",
            "Epoch 00595: val_loss improved from 431.96906 to 429.16331, saving model to model.h5\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 429.16331\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 429.16331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1EerRjbkSJ93Z2VuJdVwlRnUClVNkGBD3'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "47Rn7reHnObe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34207
        },
        "outputId": "b2c697d5-f865-49fd-cf04-733bc4dbc2fa"
      },
      "cell_type": "code",
      "source": [
        "#Phase 2 of training\n",
        "import tensorflow;\n",
        "checkpoint = tensorflow.keras.callbacks.ModelCheckpoint('model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "model.load_weights('model.h5')\n",
        "model.fit(X_train, y_train, batch_size=100, epochs=1000, verbose = 0, callbacks=[checkpoint], validation_data = (X_test, y_test))\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# model.save_weights('model_weights.h5')\n",
        "weights_file = drive.CreateFile({'title' : 'final_model_weights.h5'})\n",
        "weights_file.SetContentFile(\"model.h5\")\n",
        "weights_file.Upload()\n",
        "drive.CreateFile({'id': weights_file.get('id')})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 434.43832, saving model to model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 434.43832 to 432.60840, saving model to model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 432.60840\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 432.60840\n",
            "\n",
            "Epoch 00005: val_loss improved from 432.60840 to 429.39766, saving model to model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 429.39766\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 429.39766\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 429.39766\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 429.39766\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 429.39766\n",
            "\n",
            "Epoch 00011: val_loss improved from 429.39766 to 429.13579, saving model to model.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 429.13579 to 426.33214, saving model to model.h5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 426.33214\n",
            "\n",
            "Epoch 00014: val_loss improved from 426.33214 to 420.89631, saving model to model.h5\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 420.89631\n",
            "\n",
            "Epoch 00076: val_loss improved from 420.89631 to 418.37760, saving model to model.h5\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 418.37760\n",
            "\n",
            "Epoch 00198: val_loss improved from 418.37760 to 417.76922, saving model to model.h5\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 417.76922\n",
            "\n",
            "Epoch 00276: val_loss improved from 417.76922 to 414.92940, saving model to model.h5\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 414.92940\n",
            "\n",
            "Epoch 00296: val_loss improved from 414.92940 to 414.90964, saving model to model.h5\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 414.90964\n",
            "\n",
            "Epoch 00348: val_loss improved from 414.90964 to 414.30338, saving model to model.h5\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 414.30338\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 414.30338\n",
            "\n",
            "Epoch 00351: val_loss improved from 414.30338 to 409.05689, saving model to model.h5\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 409.05689\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 409.05689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1ZRcnkQMU2Td9BSEijmD632bwtN28KlLR'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "2hR6S-31HlWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Taking input of test file\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "#Link :https://drive.google.com/open?id=1omPMyyVytorWoFcV6ab58a2g44S_sRPi\n",
        "file_id = '1omPMyyVytorWoFcV6ab58a2g44S_sRPi' #link to zip of test images\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('test_images.zip')\n",
        "#Link :https://drive.google.com/open?id=1s5CGEidkqP_nw3wpxjmaUDJO-Be382j_\n",
        "test_id = '1s5CGEidkqP_nw3wpxjmaUDJO-Be382j_' #link to test.csv\n",
        "myfile = drive.CreateFile({'id': test_id})\n",
        "myfile.GetContentFile('predict.csv')\n",
        "test_files = pd.read_csv('predict.csv')\n",
        "model.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oL5VDNBoI0mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "zip_ref = zipfile.ZipFile('test_images.zip', 'r')\n",
        "zip_ref.extractall('MODEL/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdeixqzAJJPO",
        "colab_type": "code",
        "outputId": "6dfe1aa9-9e08-4b3d-e431-1917092674eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as _Imgdis\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "folder = \"/content/MODEL/test_images/\"\n",
        "test_files.head()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y1</th>\n",
              "      <th>y2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1474723840903DSC08089.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1473231475010DeeplearnS11276.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JPEG_20161205_135307_1000155917326.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JPEG_20160711_123440_1000518778437.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JPEG_20160803_115329_100034020722.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               image_name  x1  x2  y1  y2\n",
              "0               1474723840903DSC08089.png NaN NaN NaN NaN\n",
              "1        1473231475010DeeplearnS11276.png NaN NaN NaN NaN\n",
              "2  JPEG_20161205_135307_1000155917326.png NaN NaN NaN NaN\n",
              "3  JPEG_20160711_123440_1000518778437.png NaN NaN NaN NaN\n",
              "4   JPEG_20160803_115329_100034020722.png NaN NaN NaN NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BaBl5BahJx-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "# image_width = 160\n",
        "# image_height = 120\n",
        "# channels = 3\n",
        "dataset = np.ndarray(shape=(len(test_files), image_height, image_width, channels),\n",
        "                     dtype=np.float32)\n",
        "i = 0\n",
        "for file in test_files.loc[:, \"image_name\"]:\n",
        "    img = load_img(folder + \"/\" + file)  # this is a PIL image\n",
        "    img.thumbnail((image_width, image_height))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((48, 64, 3))\n",
        "    x = (x - 128.0) / 128.0\n",
        "    dataset[i] = x\n",
        "    i+=1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyZNWUCaKPRp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -R /content/MODEL/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXIcUX9iKP-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -R /content/test_images.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcbDyK7OMhO9",
        "colab_type": "code",
        "outputId": "806aac39-b797-4291-92ea-913677091dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "result = model.predict(dataset)\n",
        "result = pd.DataFrame(result)\n",
        "finalresult = result.round()\n",
        "test_files.loc[:, \"x1\"] = finalresult.iloc[:, 0]\n",
        "test_files.loc[:, \"x2\"] = finalresult.iloc[:, 1]\n",
        "test_files.loc[:, \"y1\"] = finalresult.iloc[:, 2]\n",
        "test_files.loc[:, \"y2\"] = finalresult.iloc[:, 3]\n",
        "test_files.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y1</th>\n",
              "      <th>y2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1474723840903DSC08089.png</td>\n",
              "      <td>236.0</td>\n",
              "      <td>457.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>408.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1473231475010DeeplearnS11276.png</td>\n",
              "      <td>79.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>346.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JPEG_20161205_135307_1000155917326.png</td>\n",
              "      <td>129.0</td>\n",
              "      <td>509.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>436.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JPEG_20160711_123440_1000518778437.png</td>\n",
              "      <td>211.0</td>\n",
              "      <td>451.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>424.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JPEG_20160803_115329_100034020722.png</td>\n",
              "      <td>140.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>428.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               image_name     x1     x2     y1     y2\n",
              "0               1474723840903DSC08089.png  236.0  457.0   86.0  408.0\n",
              "1        1473231475010DeeplearnS11276.png   79.0  565.0  140.0  346.0\n",
              "2  JPEG_20161205_135307_1000155917326.png  129.0  509.0   42.0  436.0\n",
              "3  JPEG_20160711_123440_1000518778437.png  211.0  451.0   80.0  424.0\n",
              "4   JPEG_20160803_115329_100034020722.png  140.0  515.0   41.0  428.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "Eh4upgqMQPSQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "test_files.to_csv(\"submission.csv\", index = False)\n",
        "files.download(\"submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z68fEU0yQy1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}